{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Erstelle eines bereinigten Datensatzes aus einer Webseite\n",
    "\n",
    "Prompt:\n",
    "\n",
    "Schreibe einen Python Code, der folgende Funktionen erfüllt:\n",
    "- Extrahieren aus einer Webseite alle Sätze\n",
    "- Extrahiere auch aus allen verlinkten Seiten aus der Hauptseite innerhalb der Domäne\n",
    "- max 10 links probieren\n",
    "- die Sätze sollen forbereitet werden zur embedding verarbeitung mit openai\n",
    "- Textblöcke säubern und korrigieren\n",
    "- Entfernen von Zeilenumbrüchen und Leerzeichen am Anfang und Ende\n",
    "- nur sätze nehmen die mindestens 10 Wörter hat\n",
    "- es soll als csv gespeichert werden\n",
    "- der code soll produktionsreif sein\n",
    "- der code soll eine hohe qualität aufweisen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot process https://playwright.dev/python/docs/api/class-test\n",
      "Cannot process https://playwright.dev/python/docs/api/class-test\n",
      "Cannot process https://playwright.dev/java/docs/api/class-test\n",
      "Cannot process https://playwright.dev/java/docs/api/class-test\n",
      "Cannot process https://playwright.dev/dotnet/docs/api/class-test\n",
      "Cannot process https://playwright.dev/dotnet/docs/api/class-test\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    # Entferne Zeilenumbrüche und Leerzeichen am Anfang und Ende\n",
    "    text = text.strip().replace('\\n', ' ')\n",
    "    # Entferne führende und nachfolgende Satzzeichen\n",
    "    text = text.strip(string.punctuation)\n",
    "    return text\n",
    "\n",
    "def extract_sentences(url, max_links=10):\n",
    "    sentences = []\n",
    "    visited_links = set()\n",
    "    visited_links.add(url)\n",
    "\n",
    "    def process_page(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        text_blocks = soup.find_all('p')  # Annahme: Text in Absätzen (<p>-Tags) auf der Seite\n",
    "\n",
    "        for block in text_blocks:\n",
    "            text = block.get_text()\n",
    "            # Bereinige und korrigiere den Text\n",
    "            text = clean_text(text)\n",
    "            # Teile den Text in Sätze auf\n",
    "            sentences.extend(re.split(r'[.!?]+', text))\n",
    "\n",
    "    def process_links(url, depth=0):\n",
    "        if depth >= max_links:\n",
    "            return\n",
    "\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        links = soup.find_all('a')  # Annahme: Links in <a>-Tags auf der Seite\n",
    "\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('/') and href not in visited_links:\n",
    "                visited_links.add(href)\n",
    "                new_url = url + href\n",
    "                process_page(new_url)\n",
    "                process_links(new_url, depth + 1)\n",
    "\n",
    "    process_page(url)\n",
    "    process_links(url)\n",
    "\n",
    "    # Filtere Sätze mit mindestens 10 Wörtern\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence.split()) >= 10]\n",
    "    return sentences\n",
    "\n",
    "def prepare_sentences(sentences):\n",
    "    # Vorbereitung der Sätze für OpenAI Embedding-Verarbeitung\n",
    "    # Implementiere entsprechende Vorbereitungsschritte basierend auf den Anforderungen\n",
    "\n",
    "    prepared_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Füge hier den Code für die Vorbereitung der Sätze ein\n",
    "\n",
    "        prepared_sentences.append(sentence)  # Beispiel: Füge den unveränderten Satz hinzu\n",
    "\n",
    "    return prepared_sentences\n",
    "\n",
    "def save_to_csv(sentences, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Sentence'])\n",
    "        writer.writerows([[sentence] for sentence in sentences])\n",
    "\n",
    "# Beispielaufruf\n",
    "url = 'https://www.codersunlimited.com'  # URL der Webseite, von der aus die Sätze extrahiert werden sollen\n",
    "sentences = extract_sentences(url, max_links=10)\n",
    "prepared_sentences = prepare_sentences(sentences)\n",
    "save_to_csv(prepared_sentences, 'sentences.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
